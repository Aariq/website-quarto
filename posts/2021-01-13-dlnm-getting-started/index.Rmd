---
title: 'DLNMs: building and visualizing crossbasis functions'
author: Eric R. Scott
date: '2021-01-13'
# slug: dlnm-getting-started
categories:
  - DLNMs
  - GAMs
  - r
# projects: [heliconia]
---


```{r setup, include=FALSE}
library(tidyverse)
library(here)
ha <- read_rds("ha_sample.rds")
knitr::opts_chunk$set(paged.print = FALSE)
```

::: callout-note
This is part of series about distributed lag non-linear models.
Please read the [first post](/post/dlnm) for an introduction and a disclaimer.
:::

## The dlnm package

The `dlnm` package offers two ways of fitting crossbasis functions: an "internal" and an "external" method.
The "external" method involves fitting the crossbasis function outside of a model, using some functions in the `dlnm` package, then including the results as a predictor in a model such as a generalized linear model (GLM).
I'm going to focus entirely on the "internal" method that fits the crossbasis function in the context of a generalied additive model (GAM) to take advantage of the penalization and other stuff the `mgcv` package offers.

## The data

Throughout this series, I'm going to use a subset of data from my postdoc project on *Heliconia acuminata*.
In this subset, 100 plants were tracked over a decade.
Every year in February their size was recorded as height and number of shoots, and it was recorded whether or not they flowered.
Any dead plants were marked as such.
The goal is to determine how drought impacted growth, survival, and flowering probability with a potentially delayed and/or non-linear relationship.
To that end, I've calculated SPEI, a measure of drought, where more negative numbers represent more severe drought.
SPEI is monthly while the demography data is yearly.
For every observation of a plant, there is an entire history of SPEI for the past 36 months from that observation.

```{r}
head(ha)
```

-   `plot` (factor): A plot ID
-   `ha_id_number` (factor): A unique plant ID
-   `year` (numeric): year of census
-   `size` (numeric): number of shoots x height in cm
-   `size_next` (numeric): size in the next year
-   `log_size` (numeric): log transformed size
-   `log_size_next` (numeric): log transformed size next year
-   `flwr` (numeric): Did a plant flower? 1 = yes, 0 = no
-   `surv` (numeric): Did a plant survive? 1 = yes, 0 = no
-   `spei_history` (c("matrix", "array")): A matrix column of the drought history starting in the current month (`spei_history[,1]` = February) and going back 24 months (`spei_history[,25]` = February 2 years ago)
-   `L` (c("matrix", "array")): A matrix column describing the lag structure of `spei_history`. Literally just `0:24` for every row.

## Fit a DLNM

```{r message=FALSE, warning=FALSE}
library(mgcv) #for gam()
library(dlnm) #for the "cb" basis
```

```{r}
growth <-
  gam(log_size_next ~ 
        log_size +
        s(spei_history, L, # <- the two dimensions
          bs = "cb", # <- fit as crossbasis
          k = c(3, 24), # <- knots for each dimension
          xt = list(bs = "cr")), # <- what basis to use for each dimension
      family = gaussian(link = "identity"),
      method = "REML",
      data = ha)
```

Above is a simple DLNM with survival modeled as a function of number of shoots and the crossbasis function of SPEI over the past 36 months.
`shts` is a fixed effect (i.e. not a smooth, but to be fit as a straight line), and the crossbasis is defined in `s(spei_history, L, â€¦)`.
`spei_history` and `L` are the two dimensions of the crossbasis function, `bs = "cb"` tells `gam()` that this is a crossbasis function from the `dlnm` package (calls `dlnm::smooth.construct.cb.smooth.spec` behind the scenes).
`xt = list(bs = "cr")` tells it to use a cubic regression spline as the basis for both dimensions of the crossbasis function (but you can also mix and match marginal basis functions by providing a length 2 vector here).

## Problem 1: visualizing the results

Unfortunately `plot.gam()` does not work with these crossbasis functions.

```{r error=TRUE}
plot.gam(growth)
```

The `dlnm` package provides some functions for visualizing the results of a DLNM, though I don't like them much.

First you use `crosspred()` to get predicted values for the DLNM.

```{r}
pred_dat <- crosspred("spei_history", growth)
```

Then you plot those with `plot.crosspred()`.
The default is a 3D plot.

```{r}
plot(pred_dat)
```

I prefer a heatmap, although the one produced here has some issues.

```{r}
plot(pred_dat, ptype = "contour", xlab = "SPEI", ylab = "lag(months)")
```

First obvious problem is the colors.
The range is the same for red and blue, despite different number of breaks.
Second, the units are not what I'd expect.
For a marginal effects plot these should be the size of an average plant in year t+1, all else being equal.
This is plotting the size relative to the size at an average value of SPEI, which is a weird thing to think about.
That's because the package was built with epidemiology and relative risk in mind.
Here is the plot relative to SPEI = 1.5

```{r}
pred_dat <- crosspred("spei_history", growth, cen = 1.5)
plot(pred_dat, ptype = "contour", xlab = "SPEI", ylab = "lag(months)")
```

## Solution (?)

So, I spent a lot of time writing a complicated function, `cb_margeff()`, to create data for a marginal effects plot.
It creates a `newdata` data frame to be passed to `predict()` and loops across different matrixes with all columns of `spei_history` set to average except for one, representing a range of possible SPEI values.

```{r include=FALSE}
#' Calculate marginal effects of a crossbasis smooth
#' 
#' Calculate marginal effects of a crossbasis smooth created with the `cb` basis
#' from the `dlnm` package in a model created with `gam()`. Everything is kept
#' average (or at reference value for factors) and response values are predicted
#' using the range of values of Q at each lag defined by L.
#'
#' @param Q The matrix of predictor values used to generate a crossbasis smooth
#' @param L The matrix of lags used to generate a crossbasis smooth
#' @param model a gam with a crossbasis smooth
#' @param ref_data reference data passed to the `newdata` argument of `predict`
#'   (optional). This one-row data frame should contain values for all model
#'   terms **except** `Q` and `L`.
#' @param meshpts vector of length 2; The number of meshpoints for values of Q
#'   and L, respectively, to use to generate fitted values
#' @param calc_dist logical; Calculate distance between predicted values and
#'   actual data points?  This is inspired by the `too.far` argument of
#'   `plot.gam()`.  If `TRUE` (default), it adds the column `min_dist` which is
#'   useful for filtering data before plotting because "smooths tend to go wild
#'   away from data". This step takes a long time, so if you don't need it,
#'   consider setting to `FALSE`
#' @return a tibble suitable for plotting marginal effects as a heatmap or
#'   contour plot.  `x` is the meshpoint values across the range of the
#'   predictor, `Q`; `lag` is the values of `L`; `fitted` and `se.fit` are the
#'   results of `predict.gam()`; min_dist is the euclidean distance on the unit
#'   square from the fitted values to the actual data used to fit the model.
#' @export
#' @importFrom rlang enquo abort
#' @import purrr
#' @import dplyr
#' @import mgcv
#' @import tidyr
#' @importFrom stats predict
#'
#' @examples
#' \dontrun{
#' library(dlnm)
#' library(mgcv)
#' library(tsModel)
#' data("chicagoNMMAPS")
#' Q <- Lag(chicagoNMMAPS$temp, 0:25) #temperature data, lagged
#' L <- matrix(0:25,nrow(Q),ncol(Q),byrow=TRUE) #matrix of 0-25
#' # Fit DLNM model
#' gam1 <- gam(death ~ s(Q, L, bs="cb", k=10) + s(pm10) + dow,
#'             family=quasipoisson(), 
#'             data = chicagoNMMAPS,
#'             method='REML')
#' # Calculate marginal effect of lagged temperature, all else being held average.
#' cb_margeff(Q, L, gam1)
#' }
cb_margeff <- 
  function(model, Q, L, ref_data = NULL, meshpts = c(50, 50), calc_dist = TRUE) {
    # Q_name <- quo(Q)
    # L_name <- quo(L)
    
    if (!inherits(model, "gam")) {
      abort("This is only for GAMs made with the `mgcv` package including cross-basis smooths from the `dlnm` package.")
    } 
    
    Q_name <- rlang::enquo(Q)
    L_name <- rlang::enquo(L)
    df <- model$model
    
    # Get the Q and L matrices from the model dataframe
    Q <- dplyr::pull(df, !!Q_name)
    L <- dplyr::pull(df, !!L_name)
    
    testvals <- seq(min(Q, na.rm = TRUE), max(Q, na.rm = TRUE), length.out = meshpts[1])
    Q_new <- matrix(mean(Q, na.rm = TRUE), nrow = meshpts[1], ncol = meshpts[2])
    lvals <- seq(min(L), max(L), length.out = meshpts[2])
    L_new <- matrix(lvals, nrow = meshpts[1], ncol = meshpts[2], byrow = TRUE)
    
    # For newdata, keep everything constant except varying Q.
    # Keep numeric values constant at mean.
    # Set random effects to a new level to "trick" predict().
    # Set parametric factors to reference level.
    terms_raneff <-
      model$smooth %>% 
      purrr::map_if(~inherits(.x, "random.effect"),
                    ~pluck(.x, "term"),
                    .else = function(x) return(NULL)) %>%
      purrr::compact() %>% 
      purrr::as_vector()
    
    terms_fac <- names(model$xlevels)
    
    
    if (is.null(ref_data)) {
      #TODO newdata columns must be the same class as the model data.  I think this
      #breaks if there is a fixed-effect factor put in as a character vector.
      ref_data <-
        df %>%
        dplyr::summarize(
          across(c(-!!L_name, -!!Q_name) & where(is.numeric), mean),
          across(all_of(terms_raneff) & where(is.factor), ~factor(".newdata")),
          across(all_of(terms_fac) & where(is.factor), ~factor(levels(.x)[1], levels = levels(.x)))
        )
    }
    newdata <- uncount(ref_data, meshpts[1]) %>% add_column(!!L_name := L_new)
    resp <- array(dim = c(length(testvals), ncol(Q_new)))
    rownames(resp) <- testvals
    colnames(resp) <- lvals
    se <- resp
    #loop through columns of matrix representing different lags/distances, replace
    #with testvals, predict response.
    for (i in 1:ncol(Q_new)) {
      # is there some way I can use outer() or rbind() to just make one big matrix instead of this loop?
      P1_i <- Q_new
      P1_i[, i] <- testvals
      p <- suppressWarnings( #new levels of random effects are on purpose
        predict(
          model,
          newdata = newdata %>% add_column(!!Q_name := P1_i),
          se.fit = TRUE,
          type = "link"
        )
      )
      resp[, i] <- p$fit
      se[, i] <- p$se.fit
    }
    fitted <-
      resp %>%
      dplyr::as_tibble(rownames = "x", .name_repair = "unique") %>%
      tidyr::pivot_longer(
        cols = -x,
        names_to = "lag",
        values_to = "fitted"
      ) %>%
      dplyr::mutate(lag = as.double(lag), x = as.double(x))
    
    se.fitted <-
      se %>%
      dplyr::as_tibble(rownames = "x", .name_repair = "unique") %>%
      tidyr::pivot_longer(
        cols = -x,
        names_to = "lag",
        values_to = "se.fit"
      ) %>%
      dplyr::mutate(lag = as.double(lag), x = as.double(x))
    
    pred <- dplyr::full_join(fitted, se.fitted, by = c("x", "lag"))
    if (isTRUE(calc_dist)) {
      out <- add_min_dist(df, Q_name, L_name, pred)
    } else {
      out <- pred
    }
    return(out)
  }

#' Calculate distance between predicted values and actual data points on a grid
#' 
#' This is inspired by the `too.far` argument in `plot.gam()`.  It takes
#' predicted values and adds the distance to the model data.  You can then use
#' the `min_dist` column to filter data for plotting.
#'
#' @param df data frame; model data
#' @param Q_name quosure; the name for the Q matrix
#' @param L_name quosure; the name for the L matrix
#' @param pred data frame; the predicted values
#' @import purrr
#' 
#'
#' @return a tibble
add_min_dist <- function(df, Q_name, L_name, pred) {
  d <-
    df %>%
    pull(!!Q_name) %>% 
    as_tibble(.name_repair = ~as.character(pull(df, !!L_name)[1, ])) %>% 
    pivot_longer(everything(),
                 names_to = "lag",
                 names_transform = list(lag = as.numeric),
                 values_to = "x")
  
  grid <-
    pred %>% 
    mutate(min_g_x = min(.data$x, na.rm = TRUE),
           min_g_y = min(.data$lag, na.rm = TRUE),
           g_x = .data$x - .data$min_g_x,
           g_y = .data$lag - .data$min_g_y) %>% 
    mutate(max_g_x = max(.data$g_x, na.rm = TRUE),
           max_g_y = max(.data$g_y, na.rm = TRUE),
           g_x = .data$g_x / .data$max_g_x,
           g_y = .data$g_y / .data$max_g_y)
  
  d <-
    d %>% 
    mutate(d_x = (.data$x - first(grid$min_g_x)) / first(grid$max_g_x),
           d_y = (.data$lag - first(grid$min_g_y)) / first(grid$max_g_y))
  
  #where dat is a 2-column matrix of x and y coords of the true data used to build the model
  min_dist <- function(g_x, g_y, d) {
    d[,1] <- d[,1] - g_x
    d[,2] <- d[,2] - g_y
    min(
      sqrt(
        (d[,1]^2 + d[,2]^2)
      ), na.rm = TRUE
    )
  }
  
  #I think this is the super slow step.  Would be great if it didn't have to be rowwise().
  out <-
    grid %>% 
    rowwise() %>% 
    mutate(min_dist = min_dist(g_x, g_y, cbind(d$d_x, d$d_y))) %>% 
    select(-g_x, -g_y, -max_g_x, -max_g_y, -min_g_x, -min_g_y) %>% 
    ungroup()
  
  return(out)
}
```

```{r}
plotdata <- cb_margeff(growth, spei_history, L)
ggplot(plotdata, aes(x = x, y = lag, fill = fitted)) +
  geom_raster() +
  scale_fill_viridis_c("size in year t+1", option = "A") +
  scale_x_continuous("SPEI", expand = c(0,0)) +
  scale_y_continuous("lag (months)", expand = c(0,0))
```

Yeah, this is looking better.

The interpretation of this type of plot (which I would describe as a marginal effects plot, but correct me if I'm wrong) makes more sense to me.
If there was drought (low SPEI) about 8 months prior to the census, that's bad for growth.
Drought 20 months prior is good for growth though.

**BUT WAIT**

I poked around in `plot.gam` with `debug()` and it turns out the reason the plotting doesn't work is only because the author of `dlnm`, Gasparrini, didn't want it to work.

I can change a simple flag inside the `growth` model, and then it produces something very similar (identical?) to what I have above:

```{r}
growth$smooth[[1]]$plot.me

growth$smooth[[1]]$plot.me <- TRUE
plot.gam(growth, scheme = 2)
```

Why is this default plot not available?
It's literally **exactly** what I wanted, and I'm pretty sure there's nothing incorrect about it, but it worries me that the author of `dlnm` didn't want me to make it.

# UPDATE

I think I better understand what is going on here now.
`plot.gam()`, and the `ggplot2` implementation of it, `gratia::draw()`, plot the smooth itself, not the predicted values.
By "the smooth itself", I mean the function that is acting sort of like one of the coefficients in a GLM.
Instead of $y = \beta_0 + \beta_1 x_1$, we have $y = \beta_0 + f_1(x_1)$.
To further clarify, look at the options for `predict.gam()`.
To get predicted $y$ values, you can use `type = "link"` or `type = "response"`.
But if you just want the values for $f_1(x_1)$, then you can use `type = "terms"`.
The plot above looks like the one I want, but the scale is actually not in units of plant size.
See the `gratia` version, which includes a scale bar:

```{r}
gratia::draw(growth, select = 1)
```

So my efforts in creating `cb_margeff()` weren't for nothing, afterall, and are not in conflict with the views of the `dlnm` package authors.
Some day I should probably figure out how to "manually" calculate values of $y$ from the GAM coefficients, but today is not that day. 

{{< downloadthis index.Rmd dname="index.Rmd" label="Download Rmd" >}}

