{
  "hash": "ec29e58f222d3e20a797ed4b052ad31a",
  "result": {
    "markdown": "---\ntitle: Speeding up DLNMs with bam()\n# author: Eric R. Scott\ndate: '2021-01-19'\n# slug: dlnm-bam\ncategories: \n  - DLNMs \n  - GAMs\n  - r\n---\n\n\n::: callout-note\nThis is part of series about distributed lag non-linear models.\nPlease read the [first post](/post/dlnm) for an introduction and a disclaimer.\n:::\n\nDLNMs themselves may not be *that* computationally expensive, but when combined with random effects and other smoothers, and a large-ish dataset, I've noticed `gam()` being painfully slow.\n\"Slow\" is of course relative, and I'm really only talking like a couple minutes for a model to run.\n\n`bam()` in the `mgcv` package promises to speed up fitting and predicting for GAMs on big datasets by taking advantage of parallellization through the `parallel` package.\nI'm going to try to get that working and see how much it really speeds things up.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mgcv)\nlibrary(dlnm)\nlibrary(parallel)\nlibrary(tictoc) #for simple benchmarking\n```\n:::\n\n\n\n\n## Standard gam() DLNM\n\nThis is like the DLNM I've been fitting for the last couple of blog posts except now the size covariate is fit as a smooth (`s(log_size)`) and there is a random effect of plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\ngrowth <-\n  gam(log_size_next ~ \n        s(log_size) +\n        s(plot, bs = \"re\") + #random effect\n        s(spei_history, L, #crossbasis function\n          bs = \"cb\", \n          k = c(3, 24), \n          xt = list(bs = \"cr\")),\n      family = gaussian(link = \"identity\"),\n      method = \"REML\",\n      data = ha)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.744 sec elapsed\n```\n:::\n:::\n\n\nRemember, this is just a subset of the dataset I'm working with.\nThis same model with the full dataset takes about 90 seconds to run, and if I add a second covariate of year, it takes about 380 seconds.\n\n## Set up parallization\n\n`parallel` works by running code on multiple R sessions simultaneously.\nRead the documentation before messing with this, because I think if you set the number of clusters too high, you will crash your computer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncl <- makeForkCluster()\n```\n:::\n\n\nNow, I think all I have to do is re-run the same model, just with `bam()` instead of `gam()`, and include the `cluster` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\ngrowth_bam <-\n  bam(log_size_next ~ \n        s(log_size) +\n        s(plot, bs = \"re\") + #random effect\n        s(spei_history, L, #crossbasis function\n          bs = \"cb\", \n          k = c(3, 24), \n          xt = list(bs = \"cr\")),\n      family = gaussian(link = \"identity\"),\n      method = \"REML\",\n      cluster = cl,\n      data = ha)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.636 sec elapsed\n```\n:::\n:::\n\n\nHmm..\nthat took **longer**.\nThe help file for `bam()` seems to indicate that it might not speed things up if a computationally \"expensive basis\" is used.\nSo with this small dataset, maybe it's doing more work and taking longer?\n\nWhen I switch to `bam()` for the model using the entire dataset (\\~20,000 rows), I go from 380 seconds to 41 seconds---a significant improvement!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}