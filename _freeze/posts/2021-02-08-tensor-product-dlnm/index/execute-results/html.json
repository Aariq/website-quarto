{
  "hash": "1d11590c29bf2d0aa80494867187f6a1",
  "result": {
    "markdown": "---\ntitle: Fitting a DLNM is simple without the {dlnm} package\nsubtitle: 'Wait, then what does {dlnm} do?'\nauthor: Eric R. Scott\ndate: '2021-02-08'\n# slug: [tensor-product-dlnm]\ncategories:\n  - DLNMs\n  - GAMs\n  - r\n# projects: [heliconia]\n---\n\n\n<!-- Hey, you might not bother finishing this.  You already talked about it in an earlier post on visualizing results of DLNMs -->\n\n\n\n\n\n::: callout-note\nThis is part of series about distributed lag non-linear models.\nPlease read the [first post](/post/dlnm) for an introduction and a disclaimer.\n:::\n\nSince I've been working so much with GAMs for this project, I decided to read sections of Simon Wood's book, [Generalized additive models: an introduction with R](https://www.google.com/books/edition/Generalized_Additive_Models/HL-PDwAAQBAJ?hl=en&gbpv=0) more thoroughly.\nIn Chapter 5: Smoothers, there is an example **tensor product** smooth (more on what that means later) fitting a distributed lag model.\nWhen I saw this, I started to question *everything*.\nWhat was the `dlnm` package even doing if I could fit a DLNM with just `mgcv`?\nWas it doing something *wrong*?\nWas I interpreting it wrong?\nAm I going to have to change EVERYTHING?\n\nI also found [this wonderful paper](https://cdnsciencepub.com/doi/abs/10.1139/cjfr-2018-0027) by Nothdurft and Vospernik that fits a DLNM for yearly tree ring data explained by lagged, non-linear, monthly weather data.\nThey also used only the `mgcv` package and tensor product smooths to fit this model.\nSo what is a tensor product and what is the `dlnm` package doing differently?\n\n## Tensor Products\n\nA tensor product smooth is a two (or more) dimensional smooth such that the shape of one dimension varies smoothly over the other dimension.\nTensor products are constructed from two (or more) so-called marginal smooths.\nImagine a basket weave of wood strips.\nThe strips can be different widths and be more or less flexible in each dimension.\nThe flexibility of the strips roughly corresponds to a smoothing penalty (stiffer = smoother) and the number and width of strips roughly corresponds to number of knots.\nYou can bend the first strip on one dimension, but you can't really bend the adjacent strip in the completely opposite direction.\nThe shape of the strips in one dimension is forced to vary smoothly across the other dimension.\n\n![](basket.png){fig-alt=\"basketweave pattern\" fig-align=\"center\"}\n\nThe tensor product for a DLNM has the environmental predictor on one dimension (SPEI, in our example) and lag time on the other dimension.\nSo SPEI can have a non-linear effect on plant growth, but the shape of that relationship with lag = 0 is constrained to be similar to the shape at lag = 1 (the adjacent strip of wood).\nThe change in the shape of the SPEI effect varies smoothly with lag time.\nHere's a pure `mgcv` implementation of a DLNM.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mgcv)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: nlme\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'nlme'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    collapse\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is mgcv 1.8-42. For overview type 'help(\"mgcv-package\")'.\n```\n:::\n\n```{.r .cell-code}\ngrowth_te <-\n  gam(log_size_next ~ \n        s(log_size) +\n        te(spei_history, L,\n          bs = \"cr\", \n          k = c(5, 15)),\n      family = gaussian(link = \"identity\"),\n      method = \"REML\",\n      select = TRUE,\n      data = ha)\n```\n:::\n\n\n## So what the heck is {dlnm} doing that's different?\n\nAfter re-reading Gasparrini's papers for the billionth time and reading more of Simon Wood's book, I realized the difference between the pure `mgcv` approach and the `dlnm` approach had to do with \"identifiability constraints\".\nBasically, because a GAM is a function that has covariates which themselves are functions, those smooth covariates are usually constrained to sum to 0.\nFor example, the smooth term for `s(log_size)` looks essentially centered around 0 on the y-axis when plotted.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gratia)\ndraw(growth_te, select = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nTensor product smooths in `mgcv` have this constraint as well, but not for each marginal function.\nThe entire *surface* sums to zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraw(growth_te, select = 2, dist = Inf)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe `dlnm` package constructs a \"crossbasis\" function, but it does this by using **tensor products** from `mgcv`.\nSo what is the difference?\nWell, the major difference is in how it does identifiability constraints.\nFor `te(..)`, the entire surface must sum to zero.\nFor `s(..., bs = \"cb\")`, the predictor-response dimension is constrained to sum to zero.\nThat means that every slice for any value of lag must sum to zero.\nIt also removes the the intercept from that dimension, so the resulting smooth ends up having fewer knots and fewer maximum degrees of freedom.\n\nHere's the `dlnm` version:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dlnm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is dlnm 2.4.7. For details: help(dlnm) and vignette('dlnmOverview').\n```\n:::\n\n```{.r .cell-code}\ngrowth_cb <-\n  gam(log_size_next ~ \n        s(log_size) +\n        s(spei_history, L, #crossbasis function\n          bs = \"cb\", \n          k = c(5, 15), \n          xt = list(bs = \"cr\")),\n      family = gaussian(link = \"identity\"),\n      method = \"REML\",\n      select = TRUE,\n      data = ha)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndraw(growth_cb, select = 2, dist = Inf)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nNotice how it looks much more symmetrical left to right.\nThis is more clear if we plot all the slices through lag time on top of eachother, kind of like holding the surface up with the x-axis at eye-level and looking down the y-axis in the middle.\n\n\n::: {.cell}\n\n```{.r .cell-code}\neval_cb <- evaluate_smooth(growth_cb, \"spei_history\", dist = Inf)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `evaluate_smooth()` was deprecated in gratia 0.7.0.\nℹ Please use `smooth_estimates()` instead.\n```\n:::\n\n```{.r .cell-code}\neval_te <- evaluate_smooth(growth_te, \"spei_history\", dist = Inf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\neval_cb %>% \n  #just take beginning and end\n  # filter(L == min(L) | L == max(L)) %>% \n  mutate(L = as.factor(L)) %>% \n  ggplot(aes(x = spei_history, y = est, group = L)) + \n  geom_line(alpha = 0.1) +\n  labs(title = \"crossbasis from {dlnm}\") +\n  coord_cartesian(xlim = c(-0.5,0.5), ylim = c(-0.02, 0.02)) +\n\neval_te %>% \n  #just take beginning and end\n  # filter(L == min(L) | L == max(L)) %>% \n  mutate(L = as.factor(L)) %>% \n  ggplot(aes(x = spei_history, y = est, group = L)) +\n  geom_line(alpha = 0.1) +\n  labs(title = \"tensor product from {mgcv}\") +\n  coord_cartesian(xlim = c(-0.5,0.5), ylim = c(-0.02, 0.02))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe crossbasis function intercepts are more tightly aligned, I think because of the sum-to-zero constraint along the `spei_history` axis.\n\nOddly, the slices in the `dlnm` version still don't all sum to zero, so maybe I'm still not totally explaining this right.\n\n## What does it all mean?\n\nUltimately, there is little difference between these approaches for these data.\nIf we use the `cb_margeff()` function I mentioned [earlier](post/dlnm-getting-started) on in this series to get fitted values of y (for the whole GAM, not just the smooth), then the two models look nearly identical.\n\n\n\n\n::: {.cell paged.print='false'}\n\n```{.r .cell-code}\npred_cb <- cb_margeff(growth_cb, spei_history, L)\npred_te <- cb_margeff(growth_te, spei_history, L)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pred_cb, aes(x = x, y = lag, fill = fitted)) + \n  geom_raster() +\n  geom_contour(aes(z = fitted), binwidth = 0.01, color = \"black\", alpha = 0.3) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"{dlnm} crossbasis\") +\n\nggplot(pred_te, aes(x = x, y = lag, fill = fitted)) + \n  geom_raster() +\n  geom_contour(aes(z = fitted), binwidth = 0.01, color = \"black\", alpha = 0.3) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"{mgcv} tensor product\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nAnd the summary statistics are very similar as well\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(growth_cb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nlog_size_next ~ s(log_size) + s(spei_history, L, bs = \"cb\", k = c(5, \n    15), xt = list(bs = \"cr\"))\n\nApproximate significance of smooth terms:\n                     edf Ref.df       F  p-value\ns(log_size)        1.363  9.000 261.921  < 2e-16\ns(spei_history,L)  4.570 23.000   1.077 4.01e-05\n```\n:::\n\n```{.r .cell-code}\nanova(growth_te)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nlog_size_next ~ s(log_size) + te(spei_history, L, bs = \"cr\", \n    k = c(5, 15))\n\nApproximate significance of smooth terms:\n                      edf Ref.df       F  p-value\ns(log_size)         1.364  9.000 261.905  < 2e-16\nte(spei_history,L)  4.618 26.000   0.955 4.02e-05\n```\n:::\n:::\n\n\nThe major difference, you'll notice, is that the reference degrees of freedom are larger for the tensor product version.\nAgain, that is because the crossbasis function constrains the SPEI dimension to sum to zero and the intercept along that dimension is removed.\n\n## Why {dlnm}?\n\nSo the advantages of the `dlnm` package for fitting DLNMs are probably mostly evident when you consider cases besides GAMs.\nFor example, If I wanted to constrain the lag dimension to be a switchpoint function, or if I wanted the SPEI dimension to be strictly a quadratic function, I could do that with `dlnm`.\nIf you're interested in interpreting your results in terms of relative risk ratios, then `dlnm` offers some OK visualiztion options for that.\nWhen using smooth functions for both marginal bases, the differences between using a straight tensor product with `te()` and using a crossbasis function start to fade away.\nThe `dlnm` version is still a little easier to interpret, I think, because you can more easily compare slices through lag time with the plot of the smooth itself.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}