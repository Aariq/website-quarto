{
  "hash": "2673b9f5ebf9ab7abc2b12db6915ae68",
  "result": {
    "markdown": "---\ntitle: Cupcakes vs. Muffins\nauthor: Eric R. Scott\ndate: '2018-03-05'\n# slug: cupcakes-vs-muffins\ncategories:\n  - data-science\n  - regexp\n  - r\n  - multivariate-statistics\n  - data-wrangling\n  - webscraping\n\nimage: featured.png\nimage-alt: \"A chocolate frosted chocolate cupcake (left) vs. a chocolate chip muffin (right)\"\n---\n\n\nOne thing I've learned from my PhD at Tufts is that I really enjoy working data wrangling, visualization, and statistics in R.\nI enjoy it so much, that lately I've been strongly considering a career in data science after graduation.\nAs a way to showcase my data science skills, I've been working on a side project to use web-scraping and multivariate statistics to answer the age old question: Are cupcakes really *that* different from muffins?\n\nHonestly, I can't even quite remember how this idea came to me, but it started in a discussion with Dr. Elizabeth Crone about why more ecologists don't use a statistical technique called [partial least squares regression](https://en.wikipedia.org/wiki/Partial_least_squares_regression).\nWe wanted a fun multivariate data set that could illustrate the different conclusions you might get depending on the statistical method you use.\nAround the same time, I came across a blog post by [\\@lariebyrd](https://aczane.netlify.com/2018/02/08/the-first-and-namesake-post-is-it-cake/) explaining machine learning using cake emoji.\nAnd that somehow led to me web-scraping **every single muffin and cupcake recipe** on allrecipes.com.\n\nI just finished the web-scraping bit of the project this weekend.\nI'm not going to reproduce the code here, but rather address some of the challenges I faced, and some things I've learned so far.\nYou can check out my R notebook and a .rds file of all the recipes [over on github](https://github.com/Aariq/cupcakes-vs-muffins)\n\n# Getting started on web-scraping\n\nI followed this [wonderful tutorial](https://towardsdatascience.com/web-scraping-tutorial-in-r-5e71fd107f32) from José Roberto Ayala Solares to get going.\nNecessary tools include the `tidyverse`, the `rvest` package for web-scraping, and a chrome plugin called [SelectorGadget](http://selectorgadget.com/).\nGoing through the example in the tutorial was really helpful for getting a hang of what is and isn't easy/possible.\n\n# Choosing a data source\n\nI specifically chose allrecipes.com over, say geniuskitchen.com (an excellent recipe site) because of the way it categorizes and structures recipes.\nWhen I search \"cupcake\" on geniuskitchen.com, I get 1830 results (yay!), but a bunch of them are links to videos, articles, reviews, blog posts, food porn albums, and other things that are **not** recipes (boo!).\nAllrecipes.com, on the other hand, gives me the following URL: https://www.allrecipes.com/recipes/377/desserts/cakes/cupcakes/\n\nThis is **ALL THE CUPCAKES**. From there, I played around with SelectorGadget to make sure it was going to be easy to drill down to just the links to the actual recipes, and yes, it was.\n\n![SelectorGadget in action](selector-gadget.png){fig-alt=\"Screenshot of a recipe website with a toolbar at the bottom displaying the selector \\\".fixed-recipe-card_title-link\\\".  The title/link to the recipes are highlighted on the page.\" fig-align=\"center\"}\n\nI also checked that the ingredients list of each recipe was going to be easy to scrape.\nThey all have the same format, and some brief testing convinced me that I'd be able to figure out how to pull out ingredients.\n\nThe takeaway is that for this project I had my choice of many websites, but I specifically picked one that would make my life easier because of its html structure.\n\n# Don't scrape too fast!\n\n## (`Sys.sleep()` is your friend)\n\nSo once I figured out how to pull in all the links to all the cupcake recipes, I started scraping ingredients from a small sample of them.\nAfter a few debugging runs, allrecipes.com stopped responding and I just kept getting error messages.\nAfter pulling my hair out trying to figure out how I broke my code, I realized that my IP was being blocked!\nBecause of the speed of accessing the website or how many links I accessed, my IP was suspected of being a bot or something and was temporarily (*whew*) blocked.\nI turned to twitter and was recommended an easy fix---create a custom `read_html_slow()` function that included `Sys.sleep(5)` which just makes R wait 5 seconds in between reading websites.\n\n<blockquote class=\"twitter-tweet\">\n\n<p lang=\"en\" dir=\"ltr\">\n\nMake your own function with read_html followed by Sys.sleep(5) and then map using that?\n\n</p>\n\n--- Sharon Machlis now at @smach\\@fosstodon.org (@sharon000) <a href=\"https://twitter.com/sharon000/status/965808697346789378?ref_src=twsrc%5Etfw\">February 20, 2018</a>\n\n</blockquote>\n\n\n```{=html}\n<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n```\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nread_html_slow <- function(x, ...){\n  output <- read_html(x)\n  Sys.sleep(5) #wait 5 seconds before returning output\n  return(output)\n}\n```\n:::\n\n\n# Create your own custom helper functions\n\nA great side-effect of creating your own functions like `read_html_slow()` is making your code more readable.\nInstead of a for-loop that calls `Sys.sleep(5)` after every iteration of `read_html()`, I now have one function that does it all and can be easily used in conjunction with `map()` from the `purrr` package.\n\nMy `read_html_slow()` function would still occasionally encounter errors like when it would encounter a broken URL.\nWhen reading in a whole list of URLs, one broken URL would mess up the whole list.\nI ended up expanding on `read_html_slow()` to make `read_html_safely()` which would output an `NA` rather than throwing an error if a URL was broken.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)\nread_html_safely <- possibly(read_html_slow, NA) #from the purrr package\n```\n:::\n\n\nI also created a `str_detect_any()` which allows you to check if a string is matched by any of a vector of regular expressions.\nI show how I use this in the next section\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stringi)\nstr_detect_any <- function(string, pattern){\n  map_lgl(string, ~stri_detect_regex(., pattern) %>% any(.)) \n  #map_lgl is from purrr, stri_detect() is from stringi\n}\n```\n:::\n\n\n# Work on random samples\n\nThere were something like 200 cupcake recipes and another 100 muffin recipes on allrecipes.com, which takes a long time to read in.\nRather than working on the whole data set, I used `sample()` on my vector of recipe URLs to take a manageable sample of recipes to work on.\nAfter working through a few different random subsets, I reached a point where I was happy with how my code was working.\nOnly then did I read in the entire data set.\n\n# Web-scraped data is messy\n\n## (People make weird baked goods)\n\nOnce I had all my ingredients for muffins and cupcakes in a data frame, I needed to standardize the ingredients.\nFor example \"8 tablespoons butter, melted\" and \"1/2 cup unsalted, organic, non-GMO, gluten-free, single-origin butter\" both needed to get converted to \"1/2 cup butter.\" This is where the combination of `mutate()`, `case_when()` and `str_detect()` really came in handy to make readable, debuggable code.\n\n`mutate()` is a function from the `dplyr` package (part of `tidyverse`) for adding new columns to data frames based on information in other columns.\nHere, I used it to take the ingredient descriptions and turn them into short, concise ingredients.\n`str_detect()` is from the `stringr` package and takes a string and a regular expression pattern and outputs `TRUE` or `FALSE`.\nFinally, `case_when()` is also from `dplyr` and provides a readable alternative to insane nested `ifelse()` statements.\nFor example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf <- tibble(description = c(\"1/2 cup unsalted, organic, non-GMO, gluten-free, single-origin butter\",\n                             \"1 cup buttermilk\",\n                             \"4 cups sugar\",\n                             \"4 cups slivered almonds\",\n                             \"1/2 cup chopped walnuts\",\n                             \"1 teaspoon salt\",\n                             \"25 blueberries\"))\n\n#all nuts should match one of these patterns\nnuts <- c(\"almond\", \"\\\\w*nut\", \"pecan\")\n\ndf %>%\n  mutate(ingredient = case_when(str_detect(.$description, \"butter\") ~  \"butter\",\n                                str_detect(.$description, \"milk\")   ~  \"milk\",\n                                str_detect(.$description, \"sugar\")  ~  \"sugar\",\n                                str_detect(.$description, \"salt\")   ~  \"salt\",\n                                str_detect_any(.$description, nuts) ~  \"nut\",\n                                TRUE                                ~  as.character(NA)\n                                )\n         )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 2\n  description                                                           ingred…¹\n  <chr>                                                                 <chr>   \n1 1/2 cup unsalted, organic, non-GMO, gluten-free, single-origin butter butter  \n2 1 cup buttermilk                                                      butter  \n3 4 cups sugar                                                          sugar   \n4 4 cups slivered almonds                                               nut     \n5 1/2 cup chopped walnuts                                               nut     \n6 1 teaspoon salt                                                       salt    \n7 25 blueberries                                                        <NA>    \n# … with abbreviated variable name ¹​ingredient\n```\n:::\n:::\n\n\nThe way `case_when()` works is just like a bunch of nested `ifelse()` statements.\nThat is, if it satisfies a condition on the left of the `~` in the first line, it returns the output to the right, otherwise it goes to the next line.\nThat results in \"buttermilk\" getting categorized as \"butter\".\nIf you wanted it to be captured as \"milk\" instead, you could switch the order of the butter and milk lines inside `case_when()`.\n\nI had to do this sort of thing **a lot**.\nFor example, when \"creamy peanut butter\" was getting categorized as \"cream\" or \"butter\" instead of \"nuts\" or when \"unsalted butter\" was getting categorized as \"salt\".\nYou'll also notice that if a description makes it all the way through the list, it gets categorized as `NA`.\nI'll never be able to categorize all the cupcake/muffin ingredients, because people put [weird shit](https://www.allrecipes.com/recipe/215561/peanut-butter-bacon-cupcake/) in their baked goods.\n\n# Conclusion\n\nWeb-scraping can be frustrating, but you can set yourself up for success by choosing an easily scrapeable website, annotating your code as you go, and taking measures to make your code as readable as possible.\nWith big, messy data, you'll likely never get it perfect, but you can use random samples of websites to help debug your code and test its effectiveness on new random samples of websites.\n\n# Next Steps\n\nNow that I have a data set I'm pretty happy with, the next step of the project is to do some exploratory data analysis to see what properties it has that are relevant to the sorts of multivariate data that ecologists deal with.\nThen on to statistical analyses to figure out what ingredients make cupcakes different from muffins.\nIs it sweetness?\nIs it something to do with leavening?\nButter vs oil?\nLeave your predictions in the comments below!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}