{
  "hash": "6f1feb2578c71289c6cf4a2ee8481760",
  "result": {
    "markdown": "---\ntitle: Quantifying leafhopper damage with automated supervised classification\nauthor: Eric R. Scott\ndate: '2017-12-24'\n# slug: leafhopper-FIJI\ncategories:\n  - research\n  - FIJI\n  - ImageJ\n  - tea\n  - plant-defense\n  - r\n  - leafhoppers\n\n# projects: [\"climate-leafhopper-quality\"]\n\nimage: analyzed leaf.png\nimage-alt: \"A three color image of the sillouhette of a tea leaf. The background is purple, the majority of the leaf is light green, and there are small red spots on the leaf\"\n---\n\n\n\n\nAs part of my fieldwork in China, I collected harvested tea leaves that were damaged by the tea green leafhopper.\nI want to quantify the amount of leafhopper damage for each harvest.\nI was able to find several solutions for quantifying holes in leaves or even damage to leaf margins, but typical leafhopper damage is just tiny brown spots on the undersides of leaves.\nI did find some tutorials on using [ImageJ](http://imagej.net/Welcome) to analyze diseased area on leaves, but found that the leafhopper damage spots were too small and too similar in color to undamaged leaves for these tools to work reliably and be automated.\n\n![Typical leafhopper damage](damaged%20leaf.png){fig-alt=\"a scanned image of a tea leaf on a white background.  The leaf has brown spots on it.\" fig-align=\"center\" width=\"500\"}\n\nLast year I piloted a method to quantify leafhopper damage on scanned images of tea leaves with the help of a Tufts undergraduate, Maxwell Turpin.\nWe ended up getting the most success using a supervised classification algorithm implemented by the [trainable WEKA segmentation](https://imagej.net/Trainable_Weka_Segmentation) plugin in [FIJI](https://fiji.sc/) (which stands for \"FIJI is just Image J\").\nThis semester, another Tufts undergraduate, Michelle Mu, worked on refining this approach, automating it, and applying it to the hundreds of images I obtained over the summer as part of my research.\n\n## Supervised pixel classification\n\nJust to clarify, my goal here is not image classification---that is, I'm not trying to classify leaves into categories like \"undamaged\", \"medium damaged\", \"high damage\", but rather trying to classify individual pixels in the image as being damaged or undamaged leaf tissue (or background).\n\nIn short, after selecting some pixels representative of damaged leaf, undamaged leaf, and background (regions of interest, or ROIs), the WEKA plugin trains a random forest algorithm using data from various transformations of the pixels in the ROIs.\nThen, I can apply the algorithm to other images and extract data in the form of numbers of pixels classified in each category.\n\n![Example results of WEKA classification](analyzed%20leaf.png){fig-alt=\"A three color image of the sillouhette of a tea leaf. The background is purple, the majority of the leaf is light green, and there are small red spots on the leaf\" fig-align=\"center\" width=\"500\"}\n\n## WEKA segmentation tips\n\nThe [documentation](https://imagej.net/Trainable_Weka_Segmentation) on the WEKA segmentation plugin is fairly detailed, so I won't go into great detail on how to use it, rather focus on some things I learned specific to this project.\n\n### Creating a training stack\n\nI had hundreds of images to classify, so obviously it made sense to train a classifier on a subset of leaves.\nWe started by taking my leaf scans, which contained dozens of leaves, and making images of individual leaves.\nYou can do this in any number of image manipulation software, but we found it easiest using Preview, the default image and PDF viewer on OS X. You just select a leaf with the rectangle selection tool, copy with cmd + c, and create a new image with cmd + n, then save with cmd + s.\n\nWe then chose a random subset of 15 leaves to use as a training set.\nWhy 15?\nAt the time, we were using a regular desktop computer with 8 GB of RAM, and using the WEKA plugin with an image stack any larger than that caused it to crash.\nFortunately, because the leaves were all scanned in a uniform way and leaf color didn't vary too much, 15 leaves was suitable for a training set.\nIf you have more RAM at your disposal, feel free to train on more leaves.\n\n### Training and applying a classifier\n\nWe trained the classifier on three classes, background, damaged leaf, and undamaged leaf.\nFor background and undamaged leaf, we found it was really important to focus on leaf edges, making sure to include shadows in the background class and lighter green leaf margins in the undamaged class.\nWithout doing this, the classifier would consistently mis-classify shadows and edges as either damaged or background, respectively.\nThis was also an iterative process and took several rounds of selecting ROIs, training a classifier, viewing results, and adding more ROIs.\nOnce we were satisfied with our classifier, we saved it and applied it to all of our images in stacks of 20 on a computer with 32 GB of RAM.\nThe results created by the WEKA plugin are stacks of three color images (because we used 3 classes).\nGetting numerical results turned out to be another problem.\n\n![WEKA segmentation window with ROIs selected](WEKA%20screenshot.png){fig-alt=\"User interface of the WEKA segmentation plugin with ROIs selected\" fig-align=\"center\"}\n\n### Exporting results\n\nExporting results in a numeric format turned out to be a lot more difficult than we thought it would.\nThe manual way of doing this through the FIJI menus is *Analyze \\> Histogram* which opens a histogram window, then clicking \"list\" to get a results window with the number of pixels in each class for that image, then copying and pasting into Excel.\nThis was far too labor intensive and error-prone to be appropriate for hundreds of images.\nWe needed a better way, which led us to FIJI macros.\n\nBuilding a macro turned out to be relatively painless, even though neither Michelle nor I had any experience coding in any language other than R.\nThrough a combination of forum posts and using the documentation for the [ImageJ macro language](http://imagej.net/developer/macro/functions.html) as a reference, we were able to create a macro that opens results stacks (three-color images) and exports a text file containing the number of pixels in each class for each image in the stack.\n\n``` javascript\n//ImageJ macro for exporting numerical results from classified image stacks\ninpath = getDirectory(\"Analyzed Stacks\");\nFile.makeDirectory(inpath + \"//Results//\");\n//outpath = getDirectory(\"Results\");\n//for some reason using getDirectory() twice screws things up.\n//My solution is to just create a results folder in the folder with the analyzed stacks\n//But then you get an error at the end when the script tries to open that folder.\nfiles = getFileList(inpath);\nfor(j = 0; j < lengthOf(files); j++){\n    open(files[j]);\n    title = getTitle();\n    for (n = 1; n <= nSlices(); n++) { //loop through slices\n        showProgress(n, nSlices); //this just adds a progress bar\n        setSlice(n); //set which slice\n        getStatistics(area, mean, min, max, std, histogram); //this gets the number of pixels\n        for (i=0; i<histogram.length; i++) {\n            setResult(\"Value\", i, i);\n            setResult(\"Leaf.\" + n, i, histogram[i]); //adds a column for each slice called \"Count[slicenumber]\"\n        }   \n    saveAs(\"results\", inpath + \"//Results//\" + title + \".txt\"); //saves results table as text file\n    }\n    close();\n    run(\"Clear Results\");\n}\n```\n\n### Importing Results\n\nThe results files then get read into R and tidied using a relatively simple script.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#packages you will need\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n#Get the filenames of all the results files\nfilenames <- list.files(\"Results/\") #might need to change path\n\n#Create paths to those files\nfilepaths <- paste0(\"Results/\", filenames)\n\n#make a list to eventually contain all the data files.  \nraw.list <- as.list(filenames)\nnames(raw.list) <- filenames\nraw.list\n\n#for loop for reading in every file into an element of the list.  There is probably a faster way to do this with purrr::map\nfor(i in 1:length(filenames)){   #loop through all files\n  raw.list[[i]] <- read_tsv(filepaths[i])[1:2, -1] #read only the first two rows and NOT the first column into the list\n}\nraw.list #now contains multiple data frames.\n```\n:::\n\n\nAt this point, we have created a list of data frames, one for each image stack.\nSince our image stacks were split up just to make analysis possible with limited RAM, we want to merge the results back together now.\n\nI also couldn't figure out how to rename the classes in the Image J macro (they appear as numeric labels \"0\", \"1\", and \"2\"), so we took this opportunity to rename them in our R script.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw.data <- raw.list %>%\n  bind_rows(.id = \"File\") %>% \n  mutate(Value = ifelse(Value == 0, \"damaged\", \"undamaged\") %>% as.factor()) %>% \n  #converts 0's to \"damaged\" and anything else to \"undamaged\", then converts Value to a factor\n  rename(Type = \"Value\")\n  #renames the \"Value\" column \"Type\"\n\nraw.data.2 <- raw.data %>% \n  gather(-File, -Type, key = LeafID, value = Pixels) %>%  #gathers all the data into three columns\n  spread(key = Type, value = Pixels)\n```\n:::\n\n\nAnd finished!\nWith all the image stacks analyzed using our classifier, numeric results exported using our custom macro, and then read into R and tidied using our R script, we have data ready for statistical analysis!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}